{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**SETTING UP DATA PIPELINE FOR AWS S3, AWS EC2 AND AIRFLOW DAG** </u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "\n",
    "- This instructions assumes that you already have your\n",
    "  - python script with function codes.\n",
    "  - dag file with code.\n",
    "  - You can have a look at my dag and python files at my [Github repo](https://github.com/benkeyben/10alytics_air_realtor/tree/main). If you find it helpful, please give me a star at the upper left section of my Github repo page. I'm glad you did.\n",
    "- Code blocks are **_italized_** and **_bolded_**. Copy code carefully\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Git and VS Code\n",
    "\n",
    "- Install Git on your local machine.\n",
    "- Open VS Code and navigate to your working folder where you have your dag and python files.\n",
    "- Access the terminal in VS Code by pressing **_Ctrl + Shift + backtick_**.\n",
    "- Choose Git Bash from the dropdown menu on the top right corner of the terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EC2 Instance and Key-value login in AWS console\n",
    "\n",
    "- Launch an EC2 instance, generate key-value login, and save the key-value login in your working directory.\n",
    "\n",
    "- Select **_Ubuntu OS_** and **_t2.small_** or **_t3.small_** as the instance type.\n",
    "\n",
    "- Enable SSH, HTTPS, and HTTP traffic in Network settings.\n",
    "\n",
    "- Click on Launch instance at the bottom part.\n",
    "\n",
    "- Select your new instance created and click on the connect tab.\n",
    "\n",
    "- Click on SSH client tab.\n",
    "\n",
    "- Copy the second line of code which looks like\n",
    "  **_chmod 400 your-ec2-key-value-login_name_** onto your gitbash terminal.\n",
    "\n",
    "- Copy the ssh connection line of code at the example section that starts with **_ssh -i_**.\n",
    "\n",
    "- Paste it in gitbash terminal you opened in vs code and follow the instruction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Ubuntu SSH Terminal\n",
    "\n",
    "- Update the package index with\n",
    "\n",
    "  **_sudo apt update_**\n",
    "\n",
    "- Install Python 3 package manager (Pip)\n",
    "\n",
    "  **_sudo apt install python3-pip_**\n",
    "\n",
    "- Install SQLite and Python 3.10 virtual environment module.\n",
    "\n",
    "  **_sudo apt install sqlite3_**\n",
    "\n",
    "  **_sudo apt install python3-venv_**\n",
    "\n",
    "- Create and activate a Python virtual environment.\n",
    "\n",
    "  **_python3 -m venv venv_**\n",
    "\n",
    "  **_source venv/bin/activate_**\n",
    "\n",
    "- Install Apache Airflow version 2.8.1 with PostgreSQL support.\n",
    "\n",
    "  **_pip install 'apache-airflow==2.8.1' --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.8.1/constraints-3.8.txt\"_**\n",
    "\n",
    "- Initialize Airflow metadata database and set up PostgreSQL.\n",
    "\n",
    "  **_airflow db migrate_**\n",
    "\n",
    "- Installs PostgreSQL and additional contrib packages.\n",
    "\n",
    "  **_sudo apt-get install postgresql postgresql-contrib_**\n",
    "\n",
    "- Switch to the 'postgres' user, create a database and user, and grant privileges.\n",
    "\n",
    "  **_psql_**\n",
    "\n",
    "  **_sudo -i -u postgres_**\n",
    "\n",
    "  **_CREATE DATABASE airflow;_**\n",
    "\n",
    "  **_CREATE USER airflow WITH PASSWORD 'airflow';_**\n",
    "\n",
    "  **_GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;_**\n",
    "\n",
    "- Press **_Ctrl + D_** twice to leave postgres prompt back to your ssh terminal.\n",
    "\n",
    "- Navigate to airflow directory.\n",
    "\n",
    "  **_cd airflow_**\n",
    "\n",
    "- Update Airflow configuration to use PostgreSQL.\n",
    "\n",
    "  **_sed -i 's#sqlite:////home/ubuntu/airflow/airflow.db#postgresql+psycopg2://airflow:airflow@localhost/airflow#g' airflow.cfg_**\n",
    "\n",
    "- Modify Airflow configuration to use LocalExecutor.\n",
    "\n",
    "  **_sed -i 's#SequentialExecutor#LocalExecutor#g' airflow.cfg_**\n",
    "\n",
    "- Initializes the Airflow metadata database again with the updated configuration\n",
    "\n",
    "  **_airflow db migrate_**\n",
    "\n",
    "- Create an Airflow user with administrative privileges.\n",
    "\n",
    "  **_airflow users create -u airflow -f airflow -l airflow -r Admin -e airflow@gmail.com_**\n",
    "\n",
    "- If asked for password, enter password you can remember, if possible use airflow as password (Not recommended in real prudction mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Port 8080 on EC2 Instance.\n",
    "\n",
    "- In EC2 Dashboard, select the instance and navigate to **_Security_**. If instance is not created, create it.\n",
    "\n",
    "- Locate and click on **_Security group_**.\n",
    "\n",
    "- Edit **_Inbound rules_** at the Inbound rules section. Click on **_Add rule button_**. The Inbound rules are numbered so scroll to the last one.\n",
    "  - Under **_Type_**, select Custom TCP\n",
    "  - Under **_Port range_**, type 8080\n",
    "  - Under **_Source_**, select Anywhere IPv4\n",
    "- Click on Save rules button.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Airflow Webserver and Scheduler in Ubuntu SSH Terminal\n",
    "\n",
    "- On the Ubuntu SSH terminal, run\n",
    "\n",
    "  **_airflow webserver &_**\n",
    "\n",
    "  to start the webserver\n",
    "\n",
    "- Wait for the prompt to return and run\n",
    "\n",
    "  **_airflow scheduler._**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Airflow UI\n",
    "\n",
    "- In EC2 Dashboard, select your EC2 instance.\n",
    "\n",
    "- Under **_Public IPv4 DNS_**, copy the url.\n",
    "\n",
    "- Paste it in a new browser tab, add \":8080\" at the end (e.g. **_ec2-23-43-67-23-compute-1-amazon.com:8080_**).\n",
    "\n",
    "- Log in with the Airflow username and password you created above in **_Configuring Ubuntu SSH Terminal_** section (i.e username: airflow, password: airflow).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Custom DAG Folder in Ubuntu SSH Terminal\n",
    "\n",
    "- Stop the webserver by pressing **_Ctrl + C_** twice in the Ubuntu SSH Terminal.\n",
    "\n",
    "- Navigate to the Airflow directory and create a new DAG folder to keep your dag and python files.\n",
    "\n",
    "  **_cd airflow_**\n",
    "\n",
    "  **_mkdir your_dag_folder_name_here_**\n",
    "\n",
    "- Edit the airflow.cfg file, updating the dag_folder path after the last forward slash and changing the value of Load_example to False.\n",
    "\n",
    "  **_nano airflow.cfg_**\n",
    "\n",
    "  **_dag_folder=ubuntu/airflow/your_dag_folder_name_here_**\n",
    "\n",
    "  **_Load_example=True_**\n",
    "\n",
    "- Save and exit the configuration file.\n",
    "\n",
    "  **_Ctrl + O_** and **_Ctrl + X_**\n",
    "\n",
    "- Navigate to the new DAG folder and create the DAG file and Python files and paste in the corresponding codes.\n",
    "\n",
    "  **_cd your_dag_folder_name_here_**\n",
    "\n",
    "  **_nano your_python_filename.py_**\n",
    "\n",
    "  **_nano your_dag_filename.py_**\n",
    "\n",
    "  Make sure to save and exit.\n",
    "\n",
    "- You can have a look at my dag and python files at my [Github repo](https://github.com/benkeyben/10alytics_air_realtor/tree/main)\n",
    "\n",
    "- Run\n",
    "\n",
    "  **_airflow db migrate_**\n",
    "\n",
    "  to apply changes and check for errors on the terminals.\n",
    "\n",
    "- Run the scheduler with\n",
    "\n",
    "  **_airflow scheduler_**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the Airflow UI in the web browser\n",
    "\n",
    "- Reload the airflow web page.\n",
    "\n",
    "- Check your S3 bucket to see if the data has been loaded.\n",
    "\n",
    "- Delete S3 bucket objects and terminate your EC2 instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for reading\n",
    "\n",
    "If you encounter any issues or have questions about the project or suggestions for improvement, please feel free to leave a comment.\n",
    "If you find this information helpful, please give me a big star at the upper left section of my [Github repo](https://github.com/benkeyben/10alytics_air_realtor/tree/main). I'm glad you did.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
